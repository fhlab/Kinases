{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the initialization and imports\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "from string import ascii_lowercase\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the specific functions from ind and proteins.py\n",
    "indels_path=\"/home/maya/InDelScanner\"  # /PATH/TO/InDelScanner\n",
    "if indels_path not in sys.path:\n",
    "    sys.path.append(indels_path)\n",
    "\n",
    "from ipynb.fs.defs.Library_diversity import convert_variant_to_dict, single_fraction_enrichment\n",
    "\n",
    "os.chdir(\"/mnt/c/Users/Maya/Documents/03_Kinases/\")\n",
    "\n",
    "with open('mek.pickle', 'rb') as pf:\n",
    "    mek = pickle.load(pf)\n",
    "\n",
    "with open('df_pos.pickle', 'rb') as pf:\n",
    "    df_pos = pickle.load(pf)\n",
    "    \n",
    "pos = df_pos.to_dict()\n",
    "\n",
    "# Set general restrictions stemming from SpliMLib library design\n",
    "aa_2 = ['A', 'Δ']\n",
    "aa_12 = ['A','G','P','Y','D','K','M','V','I','L','F','W']\n",
    "aa_13 = aa_12 +  ['Δ']\n",
    "splimlib = {'6': aa_12, '9': aa_12, '11': aa_12, '13': aa_12, '7a': aa_13, '8a': aa_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_ls = df_pos.index.tolist()\n",
    "active = {short : convert_variant_to_dict(short) for short in active_ls}\n",
    "valid_pos = ['6', '7a', '8a', '9', '11', '13']\n",
    "\n",
    "data = {}\n",
    "for short, m_to_pos in active.items():\n",
    "    if len(m_to_pos) != len(valid_pos):\n",
    "        continue\n",
    "    else:\n",
    "        data[short] = [m_to_pos[i] for i in valid_pos]\n",
    "\n",
    "factors = pd.DataFrame.from_dict(data, orient='index', columns=valid_pos).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors.to_pickle('mkk_factors.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = random.sample(active.items(), 1000)\n",
    "test_dict = {}\n",
    "for i in range(len(test)):\n",
    "    test_dict[test[i][0]] = test[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamming distance = 0 if the aa matches at all positions\n",
    "def hamming_distance(s1, s2, splimlib):\n",
    "    d = 0\n",
    "    if (len(s1) != len(splimlib)) or (len(s2) != len(splimlib)):\n",
    "        d += 2\n",
    "\n",
    "    for p in splimlib.keys():\n",
    "        if s1[p] != s2[p]:\n",
    "            d += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of edges\n",
    "def list_edge_nodes(active, splimlib):\n",
    "    node_pairs = []\n",
    "    for short1, s1 in active.items():\n",
    "        for short2, s2 in active.items():\n",
    "            h_dis = hamming_distance(s1, s2, splimlib)\n",
    "            if h_dis == 1:\n",
    "                node_pairs.append((short1, short2))\n",
    "    return node_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = '<?xml version=\"1.0\" encoding=\"utf-8\"?>'\n",
    "graphml_open = '<graphml xmlns=\"http://graphml.graphdrawing.org/xmlns\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \\\n",
    "    xsi:schemaLocation=\"http://graphml.graphdrawing.org/xmlns  http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd\">'\n",
    "graph_open = '<graph edgedefault=\"undirected\">'\n",
    "\n",
    "close = '</graph>\\n</graphml>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPHML functions\n",
    "\n",
    "# create nodes\n",
    "def gml_write_node(short, file):\n",
    "    print('<node id=\\\"{node}\"/>'.format(node=short), sep=\"\", file=file)\n",
    "\n",
    "def gml_list_edge_strings(node_pairs):\n",
    "    edge_strings = []\n",
    "    for pair in node_pairs:\n",
    "        edge_id = '-'.join(pair)\n",
    "        one_edge = '<edge id=\"{id}\" source=\"{s}\" target=\"{t}\"/>'.format(\n",
    "            id=edge_id, s=pair[0], t=pair[1])\n",
    "        edge_strings.append(one_edge)\n",
    "    return edge_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write the graph file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_graphml(graph_dict, filename):\n",
    "    with open (filename, 'w') as f:\n",
    "        # print the GRAPHML header definitions\n",
    "        print(header, graphml_open, graph_open, file=f, sep='\\n')\n",
    "\n",
    "        # write nodes\n",
    "        for short in graph_dict.keys():\n",
    "            gml_write_node(short, f)\n",
    "\n",
    "        # write edges\n",
    "        node_pairs = list_edge_nodes(graph_dict, splimlib)\n",
    "        edge_strings = gml_list_edge_strings(node_pairs)\n",
    "        f.writelines(edge_strings)\n",
    "\n",
    "        # end the graph and graphml\n",
    "        print(close, file=f)\n",
    "    \n",
    "    return node_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_edges = write_graphml(active, 'active.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence similarity network analysis with NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ham = nx.read_graphml('active.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVERAGE NODE DEGREE\n",
    "g_ham.size() / g_ham.order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_components = list(g_ham.subgraph(c) for c in nx.connected_components(g_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ham_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is justified to only examine the largest connected subgraph, since the next subgraphs have very few nodes: 5 (2x), 4 (4x), 3 (12x) or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_lens = {i: len(ham_components[i]) for i in range(len(ham_components))} # get the number of nodes in the largest subgraphs\n",
    "\n",
    "sort_components = sorted(component_lens.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_components[:10]:\n",
    "\tprint(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = ham_components[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.size()/G.order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence = sorted([d for n, d in G.degree()], reverse=True)  # degree sequence\n",
    "# print \"Degree sequence\", degree_sequence\n",
    "degreeCount = Counter(degree_sequence)\n",
    "deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(deg, cnt, width=0.80, color='b')\n",
    "\n",
    "plt.title(\"Largest connected subgraph\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence = sorted([d for n, d in g_ham.degree()], reverse=True)  # degree sequence\n",
    "# print \"Degree sequence\", degree_sequence\n",
    "degreeCount = Counter(degree_sequence)\n",
    "deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(deg, cnt, width=0.80, color='b')\n",
    "\n",
    "plt.title(\"All nodes\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking promising. At this point, take the largest subgraph into Gephi to build a good visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, 'Largest_connected_26563vars.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.degree('6P/9I/11L/13P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.Graph()\n",
    "# copy over the nodes from G\n",
    "for n in G.nodes():\n",
    "    H.add_node(n, deg=G.degree[n])\n",
    "for e in G.edges():\n",
    "    H.add_edge(e[0], e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = nx.clustering(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the relationship between node degree and node clustering coefficient?\n",
    "node_cluster = {}\n",
    "for n in H.nodes:\n",
    "    d = H.degree(n) # get node degree\n",
    "    c = cl[n] # get cluster coeffiecient\n",
    "    try:\n",
    "        node_cluster[d].append(c)\n",
    "    except KeyError:\n",
    "        node_cluster[d] = [c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the clustering coefficients\n",
    "nc_avg = OrderedDict()\n",
    "for deg in sorted(node_cluster):\n",
    "    nc_avg[deg] = sum(node_cluster[deg])/len(node_cluster[deg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# build a box plot\n",
    "ax.boxplot(list(node_cluster.values()))\n",
    "ax.plot(deg[1:], cnt[1:], color='b')\n",
    "ax.set_xlabel(\"Node degree\")\n",
    "ax.set_ylabel(\"Distribution of the clustering coeffiecient\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg, cnt = zip(*nc_avg.items())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(deg[1:], cnt[1:], color='b')\n",
    "plt.ylim([0, 0.3])\n",
    "plt.ylabel(\"Average clustering coefficient\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start experimenting with pruning.\n",
    "\n",
    "https://stackoverflow.com/questions/18381187/functions-for-pruning-a-networkx-graph\n",
    "\n",
    "One option is to try pruning nodes according to betweeness centrality. First, calculate the betweeness centrality for all nodes, identifying the ones that are 'hubs': appear in many shortest paths between nodes. Plot a distribution on betcen to get an idea of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closeness centrality\n",
    "clo_cen = nx.closeness_centrality(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H_closeness_centrality.pickle', 'wb') as f:\n",
    "    pickle.dump(clo_cen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness centrality\n",
    "bet_cen = nx.betweenness_centrality(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H_betweenness_centrality.pickle', 'wb') as f:\n",
    "    pickle.dump(clo_cen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_cen = nx.eigenvector_centrality(H)\n",
    "\n",
    "with open('H_eigenvector_centrality.pickle', 'wb') as f:\n",
    "    pickle.dump(eig_cen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(H, clo_cen, 'closeness')\n",
    "nx.set_node_attributes(H, bet_cen, 'betweenness')\n",
    "nx.set_node_attributes(H, eig_cen, 'eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(H.degree(H.nodes()))\n",
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)\n",
    "print(\"Top 20 nodes by degree:\")\n",
    "for d in sorted_degree[:20]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.diameter(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_short = '6P/9I/11L/13P'\n",
    "wt = {'6': 'P', '7a': 'Δ', '8a': 'Δ', '9': 'I', '11': 'L', '13': 'P'}\n",
    "\n",
    "H_dist = {}\n",
    "for n in H.nodes():\n",
    "    s1 = convert_variant_to_dict(n)\n",
    "    H_dist[n] = hamming_distance(wt, s1, splimlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(H, H_dist, \"Hamming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(H, 'connected_annotated.graphml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
